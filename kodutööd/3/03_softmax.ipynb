{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f4ec72",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "- Code must be commented. If you use code you found online, you have to add the link to the source you used. There is no penalty for using outside sources as long as you convince us you understand the code.\n",
    "\n",
    "**To pass the homework you need to attempt to do all tasks. If we see that you misunderstand some concept, you may receive personal feedback and you can have a chance to resubmit your homework.**\n",
    "**Please treat these homeworks as learning opportunities to support your study! Instead of copy-pasting solution, attempt to do it yourself and rather learn from feedback**\n",
    "\n",
    "\\*Once completed zip the entire directory containing this exercise and upload it to Moodle.\n",
    "\n",
    "**For background reading see http://cs231n.github.io/linear-classify/ and http://cs231n.github.io/optimization-1/.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec57bf",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "We have a linear classifier (you could also call it 1-layer neural network). Input to the network is a vector $\\mathbf{x}$ of $D$ features, output of the network is vector of $C$ class probablities $\\mathbf{p}$. The target class $c$ is coded as one-hot vector $\\mathbf{y}$ (meaning it has 1 at index $c$ and zeroes everywhere else). Weights of the network are represented by $N \\times C$ matrix $\\mathbf{W}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{pmatrix} x_1 & x_2 & ..& x_D \\end{pmatrix}\n",
    "\\qquad\n",
    "\\mathbf{p} = \\begin{pmatrix} p_1 & p_2 & .. & p_C \\end{pmatrix}\n",
    "\\qquad\n",
    "\\mathbf{y} = \\begin{pmatrix} y_1 & y_2 & .. & y_C \\end{pmatrix}\n",
    "\\qquad y_i =\n",
    "\\begin{cases}\n",
    "    1, &\\textrm{if}\\ \\ i=c\\\\\n",
    "    0, &otherwise\n",
    "\\end{cases}\n",
    "\\qquad\n",
    "\\mathbf{W} = \\begin{pmatrix} w_{11}&w_{12}&..&w_{1C}\\\\w_{21}&w_{22}&..&w_{2C}\\\\..&..&..&..\\\\w_{D1}&w_{D2}&..&w_{DC} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Notice we are missing **biases**, but without a loss of generalization we can add another feature to the inputs which is always $1$ and this turns weights of this feature into biases.\n",
    "\n",
    "To train the network we use cross-entropy loss function and perform gradient descent with respect to weight matrix $\\mathbf{W}$. Following represents step-by-step forward pass of the network, where $L$ is the loss function:\n",
    "\n",
    "\\begin{align*}\n",
    "z*j = \\sum*{i=1}^D x*i W*{ij},\\qquad\\qquad \\textrm{which can be achieved via} \\qquad\\qquad \\mathbf{z} &= \\mathbf{x}W,\n",
    "\\\\\n",
    "\\qquad p*i = \\frac{e^{z_i}}{\\sum*{j=1}^C e^{z_j}},\\qquad\\qquad\\qquad \\textrm{ or in short} \\qquad\\qquad\\qquad\\qquad \\mathbf{p} &= softmax(\\mathbf{z}),\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "L &= -\\sum\\_{i=1}^C y_i \\log p_i = -\\log p_c \\qquad\\qquad\\qquad \\textrm{ (only c-th element matters)}\n",
    "\\end{align*}\n",
    "\n",
    "Graphically this problem looks like:\n",
    "\n",
    "<img src=\"network.png\" width=\"750\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb3ace",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\delta L}{\\delta p_l} &= -\\frac{\\delta}{\\delta p_l}\\sum_{i=1}^C y_{i} \\log p_i  = -\\frac{y_l}{p_l}\n",
    "\\\\\n",
    "\\frac{\\delta p_l}{\\delta z_k} &= \\frac{\\delta \\frac{e^{z_l}}{\\sum_{j=1}^m e^{z_j} } }{\\delta z_k} =\n",
    "\\begin{cases}\n",
    "   \\frac{e^{z_l} \\sum_{j=1}^m e^{z_j}-e^{z_l} e^{z_l}}{\\left(\\sum_{j=1}^m\\ e^{z_j}\\right)^2} = \\frac{e^{z_l}}{\\sum_{j=1}^m\\ e^{z_j}} \\left(\\frac{\\sum_{j=1}^m e^{z_j}}{\\sum_{j=1}^m\\ e^{z_j}} - \\frac{e^{z_l}}{\\sum_{j=1}^m\\ e^{z_j}} \\right) = p_l(1-p_l), & l=k\\\\\n",
    "   \\frac{-\\ e^{z_l} e^{z_k}}{\\left(\\sum_{j=1}^m\\ e^{z_j}\\right)^2} = -p_l p_k, & l\\neq k\n",
    "\\end{cases}\n",
    "\\\\\n",
    "\\frac{\\delta z_k}{\\delta W_{ij}} &= \\frac{\\delta \\sum_{i=1}^D x_{i}W_{ik}}{\\delta W_{ij}}=  x_i\\\\\n",
    "or\n",
    "\\\\\n",
    "\\frac{\\delta z_k}{\\delta W_{ij}} &= \\frac{\\delta \\sum_{i=1}^D \\sum_{j=1}^C x_{i}W_{ij}}{\\delta W_{ij}}=\n",
    "\\begin{cases}\n",
    "  x_i, & j=k\\\\\n",
    "   0, & j\\neq k\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045ef870",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "For you to be able to complete the coding tasks, we give you the correct final formulas, obtained when putting the above three partial derivatives together:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial z_k} &= \\sum_{l=1}^C \\frac{\\partial L}{\\partial p_l} \\frac{\\partial p_l}{\\partial z_k}= -\\frac{y_k}{p_k} p_k (1-p_k) + \\sum_{l\\neq k}^C \\frac{y_l}{p_l} p_l p_k = -y_k + y_k p_k + \\sum_{l\\neq k}^C y_l p_k  = -y_k + p_k \\sum_{l=1}^C y_l= p_k-y_k\n",
    "\\\\\n",
    "\\frac{\\partial L}{\\partial W_{ij}} &= \\sum_{k=1}^C \\frac{\\partial L}{\\partial z_k} \\frac{\\partial z_k}{\\partial W_{ij}} = (p_j - y_j) x_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This computes the gradient value in one data point. However, for increased stability, it is beneficial to learn from average gradient over multiple points. The collection of N samples is called a **minibatch**. We note the loss in k-th data point by $L_k$ and the average loss with $\\textbf{L}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\textbf{L}=\\frac{1}{N}\\sum\\_{k=1}^N L_k\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\textbf{L}}{\\partial W*{ij}} &= \\frac{1}{N} \\sum*{l=1}^N \\frac{\\partial L*k}{\\partial W*{ij}}\n",
    "\\end{align*}\n",
    "\n",
    "It is convenient to work with average loss and average gradients, not the sum, because then the magnitude of the value does not depend on batch-size N.\n",
    "\n",
    "---\n",
    "\n",
    "To perform a gradient descent you need to subtract gradient from the weights (because we are minimizing the loss function):\n",
    "\n",
    "$$\n",
    "W_{ij}^{t+1} = W_{ij}^t - \\alpha \\frac{\\partial \\textbf{L}}{\\partial W_{ij}}\n",
    "$$\n",
    "\n",
    "Here $\\alpha$ is a learning rate that must be tuned manually.\n",
    "\n",
    "Using the learning rule and the formulas for calculating gradients for each element of W, you should be able to complete the first coding task below (marked with task 2.2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0959f",
   "metadata": {},
   "source": [
    "## CIFAR-10 Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff71d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  torch.Size([50000, 32, 32, 3])\n",
      "Training labels shape:  torch.Size([50000])\n",
      "Test data shape:  torch.Size([10000, 32, 32, 3])\n",
      "Test labels shape:  torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "    Load CIFAR (check that the path to dataset directory matches its location in your computer.)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "cifar10_dir = '../cifar-10-batches-py/'\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# move top torch\n",
    "X_train = torch.from_numpy( X_train ).float()\n",
    "y_train = torch.from_numpy( y_train ).float()\n",
    "X_test = torch.from_numpy( X_test ).float()\n",
    "y_test = torch.from_numpy( y_test ).float()\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "    Visualize some examples from the dataset.\n",
    "    We show a few examples of training images from each class.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "classes = [\n",
    "    'plane',\n",
    "    'car',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck'\n",
    "]\n",
    "\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "\n",
    "for y, cls in enumerate(classes):\n",
    "\n",
    "    idxs = torch.where(y_train == y)[0]\n",
    "\n",
    "    perm = torch.randperm(len(idxs))[:samples_per_class] # permutation\n",
    "    selected_idxs = idxs[perm]\n",
    "\n",
    "    for i, idx in enumerate(selected_idxs):\n",
    "\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "\n",
    "        img = X_train[idx]\n",
    "\n",
    "        plt.imshow(img.to(torch.uint8).cpu().numpy()) # tensor to cpu\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824420b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "    Split the data into train, val, and test sets. In addition we will\n",
    "    create a small development set as a subset of the training data;\n",
    "    we can use this for development so our code runs faster.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "X_val = X_train[ num_training : num_training + num_validation ]\n",
    "y_val = y_train[ num_training : num_training + num_validation ]\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "#X_train_original = X_train # Keep a reference if needed\n",
    "X_train = X_train[ : num_training ]\n",
    "y_train = y_train[ : num_training ]\n",
    "\n",
    "# We will also make a development set, which is a small subset of\n",
    "# the training set.\n",
    "mask = torch.randperm(num_training)[ : num_dev ] # permutation\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]\n",
    "\n",
    "# We use the first num_test points of the original test set as our\n",
    "# test set.\n",
    "X_test = X_test[:num_test]\n",
    "y_test = y_test[:num_test]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db01eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "    Preprocessing: reshape the image data into rows\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "X_dev = X_dev.reshape(X_dev.shape[0], -1)\n",
    "\n",
    "print( f'X train shape: { X_train.shape }' )\n",
    "print( f'X val shape: { X_val.shape }' )\n",
    "print( f'X test shape: { X_test.shape }' )\n",
    "print( f'X dev shape: { X_dev.shape }' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "    Preprocessing: subtract the mean image\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# first: compute the image mean based on the training data\n",
    "\n",
    "mean_image = torch.mean(X_train, dim=0)\n",
    "print(mean_image[:10])\n",
    "\n",
    "fig, ax = plt.subplots( 1, 2 )\n",
    "\n",
    "ax[0].imshow( X_train[0].reshape(32, 32, 3).to(torch.uint8).cpu().numpy() )\n",
    "ax[1].imshow( mean_image.reshape(32, 32, 3).to(torch.uint8).cpu().numpy() )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5991cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second: subtract the mean image from train and test data\n",
    "\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f08f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third: append the bias dimension of ones (i.e. bias trick) so that our model only has to worry about optimizing a single weight matrix W.\n",
    "\n",
    "X_train = torch.cat([X_train, torch.ones((X_train.shape[0], 1))], dim=1)\n",
    "X_val = torch.cat([X_val, torch.ones((X_val.shape[0], 1))], dim=1)\n",
    "X_test = torch.cat([X_test, torch.ones((X_test.shape[0], 1))], dim=1)\n",
    "X_dev = torch.cat([X_dev, torch.ones((X_dev.shape[0], 1))], dim=1)\n",
    "\n",
    "print( f'X train shape: { X_train.shape }' )\n",
    "print( f'X val shape: { X_val.shape }' )\n",
    "print( f'X test shape: { X_test.shape }' )\n",
    "print( f'X dev shape: { X_dev.shape }' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a6d8a",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `softmax.py`.\n",
    "\n",
    "**Task 3.1:** First implement the naive softmax loss function with nested loops. Open the file `softmax.py` and implement the `softmax_loss_naive` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a457de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#                                                                           #\n",
    "# TODO: implement code in softmax.py                                        #\n",
    "#                                                                           #\n",
    "#############################################################################\n",
    "\n",
    "from softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = torch.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % ( -torch.log(torch.tensor(0.1)) ))\n",
    "\n",
    "assert loss != 0.0, \"ERROR: Loss is zero - softmax_loss_naive seems to not be implemented\"\n",
    "assert torch.isclose(loss, -torch.log(torch.tensor(0.1, dtype=torch.float64)), atol=0.5), \\\n",
    "    \"ERROR: Loss is not close to -log(0.1) = 2.302585, check your softmax implementation\"\n",
    "print(\"Task 3.1 passed: loss sanity check OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea493e",
   "metadata": {},
   "source": [
    "**Task 3.2:** Why do we expect our loss to be close to $-\\log(0.1)$? Explain briefly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ffe8a",
   "metadata": {},
   "source": [
    "**Your answer:** Because the weights are random with very small differences between them, the vector $\\mathbf{p}$ will be approximately uniform distribution (because the elements of $\\mathbf{z}$ will be almost equal). Since we have 10 classes, the probability of each class is approximately $0.1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f6cf3",
   "metadata": {},
   "source": [
    "**Task 3.3:** Complete the implementation of `softmax_loss_naive` and implement a (naive) version of the gradient that uses nested loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf37c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#                                                                           #\n",
    "# TODO: implement code in softmax.py                                        #\n",
    "#                                                                           #\n",
    "#############################################################################\n",
    "\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# Use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient\n",
    "# (relative error should be in the magnitude of 1e-8).\n",
    "print(\"Without regularization:\")\n",
    "from gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# Do another gradient check with regularization.\n",
    "print(\"With regularization:\")\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# Hint: the gardients your code finds and the gradients computed numerically should more-or-less agree\n",
    "# you can expect on average 10^-8 error. A 10^-6 might also happen. Anything bigger is suspicious.\n",
    "\n",
    "assert not torch.all(grad == 0), \\\n",
    "    \"ERROR: Gradient is all zeros - gradient computation in softmax_loss_naive seems not implemented\"\n",
    "print(\"Task 3.3 passed: gradient is non-zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2186f4b",
   "metadata": {},
   "source": [
    "## Vectorization of the network and learning\n",
    "\n",
    "By now you have (hopefully) implemented how to do the froward pass and apply the gradient calculation element by element to the parameters in the weight matrix. However, this code performed the gradient comutation for one data point at a time, with a surrounding for-loop collecting values over N data points and later averaging them.\n",
    "\n",
    "As demonstrated on distance calculations in the previous practice, calculating values using for loops is time-consuming. We now look for a way to get rid of for-loops and compute values in a vecotrized way for all N data points in one go.\n",
    "\n",
    "We now have as inputs:\n",
    "\n",
    "**X:** Matrix of shape (N, D) containing a minibatch of data.  \n",
    "**W:** Matrix of shape (D, C) containing weights.  \n",
    "The correct labels can be cosidered as one-hot encoded, forming also a matrix:  \n",
    "**Y:** Matrix of shape (N,C) containing the labels\n",
    "\n",
    "---\n",
    "\n",
    "First of all, we look at the **forward pass.**\n",
    "\n",
    "Above, we showed that given one row vector of inputs (shape (1,D)), the weighted summing corresponds to the matrix multiplication:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{z} &= \\mathbf{x}W \\qquad\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This also holds for a matrix of inputs where each line is a data point (N,D):\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{Z} &= \\mathbf{X}W,\n",
    "\\end{align*}\n",
    "\n",
    "which can be shown on whiteboard. The result is a NxC matrix of **Z**, **where again each line of activations ccorresponds to a data point.** The softmax operation is also applied line-by-line, so each line of P corresponds to one input. Finally, the cross-entropy loss calculation is done line-by-line resulting in a loss value per data point - shape (N,1).\n",
    "\n",
    "So we have  \n",
    "**Z:** Matrix of shape (N, C) containing activation vectors.  \n",
    "**P:** Matrix of shape (N, C) containing output probabilities.  \n",
    "**L:** Vector of shape (N, 1) containing losses.\n",
    "\n",
    "We wish to average over the losses as in the naive implementation.\n",
    "\n",
    "---\n",
    "\n",
    "**Backward pass:** Secondly, we need to calculate the gradients with respect to the weights.\n",
    "\n",
    "In our above notation, where L was a real number and x was a vector, we had:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial W\\_{ij}} &= (p_j - y_j) x_i\n",
    "\\end{align*}\n",
    "\n",
    "In here, X is (N,D) matrix. We showed above that each line of Z, P and Y correspond to one data point and depend on only one line of X. So, we can express average gradient over samples as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\textbf{L}}{\\partial W_{ij}} &= \\frac{1}{N} \\sum_{k=1}^N x_{ki}(p_{kj}-y_{kj})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This sum can be achieved via the following matrix multiplication:  \n",
    "\\begin{align*}\n",
    "\\frac{1}{N}\\textbf{X}^T \\ (P \\ - \\ Y) &=\\frac{1}{N} \\sum*{k=1}^N x*{ki}(p*{kj}-y*{kj})\n",
    "\\end{align*}\n",
    "\n",
    "The correspondence can be showed on whiteboard in class, if needed.\n",
    "\n",
    "The matrix multiplication formulas for finding **Z**, and $\\frac{\\partial \\textbf{L}}{\\partial W_{ij}}$, combined with some broadcasting, are sufficient to implement the forward and backward passes without for loops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886eecb",
   "metadata": {},
   "source": [
    "**Task 3.4:** Now that we have a naive implementation of the softmax loss function and its gradient, implement a vectorized version in `softmax_loss_vectorized`. The two versions should compute the same results, but the vectorized version should be much faster (around 10x).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf61741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#                                                                           #\n",
    "# TODO: implement code in softmax.py                                        #\n",
    "#                                                                           #\n",
    "#############################################################################\n",
    "\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# We use the Frobenius norm to compare the two versions of the gradient.\n",
    "\n",
    "grad_difference = torch.norm(grad_naive - grad_vectorized, p='fro').item()\n",
    "\n",
    "print('Loss difference: %f' % abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)\n",
    "\n",
    "#Hint, these two differences have to be 0.0000, otherwise you must have a bug.\n",
    "\n",
    "assert abs(loss_naive - loss_vectorized) < 1e-3, \\\n",
    "    \"ERROR: Loss difference between naive and vectorized is too large - check vectorized implementation\"\n",
    "assert grad_difference < 1e-3, \\\n",
    "    \"ERROR: Gradient difference between naive and vectorized is too large - check vectorized implementation\"\n",
    "print(\"Task 3.4 passed: vectorized implementation matches naive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729953d3",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "We now have vectorized and efficient expressions for the loss, the gradient and our gradient matches the numerical gradient. We are therefore ready to do SGD to minimize the loss.\n",
    "\n",
    "**Task 3.5**\n",
    "\n",
    "In the file `linear_classifier.py`, implement SGD in the function `LinearClassifier.train()` and then run it with the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f73306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#                                                                           #\n",
    "# TODO: implement code in linear_classifier.py                              #\n",
    "#                                                                           #\n",
    "#############################################################################\n",
    "\n",
    "from linear_classifier import Softmax\n",
    "\n",
    "# Assuming X_train and y_train are already torch tensors\n",
    "model = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = model.train(X_train, y_train,\n",
    "                        learning_rate=1e-7,\n",
    "                        reg=2.5e4,\n",
    "                        num_iters=1500,\n",
    "                        verbose=True)\n",
    "\n",
    "toc = time.time()\n",
    "\n",
    "print('That took %fs' % (toc - tic))\n",
    "\n",
    "assert len(loss_hist) > 0, \"ERROR: No loss history recorded - training loop seems not implemented\"\n",
    "assert loss_hist[-1] < loss_hist[0], \\\n",
    "    \"ERROR: Loss did not decrease during training - check SGD implementation in linear_classifier.py\"\n",
    "print(\"Task 3.5 passed: training loss decreased from %.4f to %.4f\" % (loss_hist[0], loss_hist[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A useful debugging strategy is to plot the loss as a function of\n",
    "# iteration number:\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d66ba6e",
   "metadata": {},
   "source": [
    "**Task 3.6**\n",
    "\n",
    "Write the `LinearClassifier.predict` function and evaluate the performance on both the training and validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#                                                                           #\n",
    "# TODO: implement code in linear_classifier.py                              #\n",
    "#                                                                           #\n",
    "#############################################################################\n",
    "\n",
    "y_train_pred = model.predict(X_train)\n",
    "train_acc = torch.mean((y_train == y_train_pred).float()).item()\n",
    "print('training accuracy: %f' % train_acc)\n",
    "y_val_pred = model.predict(X_val)\n",
    "val_acc = torch.mean((y_val == y_val_pred).float()).item()\n",
    "print('validation accuracy: %f' % val_acc)\n",
    "#hint, if everything is correct, the results should be in range 0.3 to 0.35\n",
    "assert train_acc > 0.15, \"ERROR: Training accuracy too low - check predict() in linear_classifier.py\"\n",
    "assert val_acc > 0.15, \"ERROR: Validation accuracy too low - check predict() in linear_classifier.py\"\n",
    "print(\"Task 3.6 passed: training accuracy: %.4f, validation accuracy: %.4f\" % (train_acc, val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ba16b",
   "metadata": {},
   "source": [
    "**Task 3.7**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f46ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges (orders of magnitude)\n",
    "# for the learning rates and regularization strengths.\n",
    "#\n",
    "# YOUR TASK IS to get a classification accuracy of over 0.35 on the validation set.\n",
    "\n",
    "from linear_classifier import Softmax\n",
    "\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "X_train = X_train.to(torch.float64)\n",
    "X_val = X_val.to(torch.float64)\n",
    "y_train = y_train.to(torch.long)\n",
    "y_val = y_val.to(torch.long)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For each combination of hyperparameters, train a linear model on the    #\n",
    "# training set, compute its accuracy on the training and validation sets, and  #\n",
    "# store these numbers in the results dictionary. In addition, store the best   #\n",
    "# validation accuracy in best_val and the Softmax object that achieves this    #\n",
    "# accuracy in best_softmax.                                                    #\n",
    "# TODO: the best model must achieve at least 0.35 accuracy                     #\n",
    "# TODO: if not all parameters you tried are visible in the final submitted     #\n",
    "# version, you might add a comment about what else you tried earlier.          #\n",
    "#                                                                              #\n",
    "# Hint: You should use a small value for num_iters as you develop your hyper-  #\n",
    "# parameter search code so that the models don't take much time to train; once #\n",
    "# you are confident that your code works, you should rerun the code with a     #\n",
    "# larger value for num_iters.                                                  #\n",
    "# The same approach might be useful to first determine the coarse range of     #\n",
    "# useful values for both parameters (should it be near 0.1 or 0.0001?) and then#\n",
    "# perform a finer-grained search.                                              #\n",
    "################################################################################\n",
    "\n",
    "learning_rates = [] #fill this with LR you want to try, good LRs are quite small\n",
    "regularization_strengths = [] #the regs you want to try should be bigger\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "\n",
    "# Print out results.\n",
    "print(\"Hyperparameter tuning results:\")\n",
    "for (lr, reg), accuracies in results.items():\n",
    "    print(f'lr {lr:e} reg {reg:e} train accuracy: {accuracies[\"train_accuracy\"]:.3f} val accuracy: {accuracies[\"val_accuracy\"]:.3f}')\n",
    "\n",
    "print(f'Best validation accuracy achieved: {best_val:.3f}')\n",
    "\n",
    "assert best_val >= 0.35, \\\n",
    "    \"ERROR: Best validation accuracy %.3f is below required 0.35 - try different hyperparameters\" % best_val\n",
    "print(\"Task 3.7 passed: best validation accuracy %.3f >= 0.35\" % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ac73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cross-validation results\n",
    "import math\n",
    "\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# plot training accuracy\n",
    "marker_size = 100\n",
    "colors = [results[x][\"train_accuracy\"] for x in results]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 training accuracy')\n",
    "\n",
    "# plot validation accuracy\n",
    "colors = [results[x][\"val_accuracy\"] for x in results] # default size of markers is 20\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3767c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "X_test = X_test.to(best_softmax.W.device).to(torch.float64)\n",
    "y_test = y_test.to(best_softmax.W.device).to(torch.long)\n",
    "\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "\n",
    "test_accuracy = torch.mean((y_test == y_test_pred).float()).item()\n",
    "\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1, :] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = torch.min(w), torch.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "\n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.type(torch.uint8).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd64ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
